{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Ridge Regression is a regularized version of Linear Regression: a regularization term equal to $\\alpha \\sum_{i=1}^n \\theta_i^2$ is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model's performance using the unregularized performance measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Regularization` is a technique used in machine learning and statistics to prevent overfitting of models on training data. Overfitting occurs when a model learns the training data too well, including its noise and outliers, leading to poor generalization to new, unseen data. Regularization helps to solve this problem by adding a penalty to the model's complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression, also known as Tikhonov regularization, is a type of linear regression that includes a regularization term. The key idea behind ridge regression is to find a new line that doesn't fit the training data as well as ordinary least squares regression, in order to achieve better generalization to new data. This is particularly useful when dealing with multicollinearity (independent variables are highly correlated) or when the number of predictors (features) exceeds the number of observations.\n",
    "\n",
    "### Key Concept:\n",
    "\n",
    "- **Regularization**: Ridge regression adds a penalty equal to the square of the magnitude of coefficients. This penalty term (squared L2 norm) shrinks the coefficients towards zero, but it doesn't make them exactly zero.\n",
    "\n",
    "### Mathematical Representation:\n",
    "\n",
    "The ridge regression modifies the least squares objective function by adding a penalty term:\n",
    "\n",
    "$$ \\text{Minimize } \\sum*{i=1}^{n} (y_i - \\sum*{j=1}^{p} x*{ij} \\beta_j)^2 + \\lambda \\sum*{j=1}^{p} \\beta_j^2 $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ y_i $ is the response value for the ith observation.\n",
    "- $ x\\_{ij} $ is the value of the jth predictor for the ith observation.\n",
    "- $ \\beta_j $ is the regression coefficient for the jth predictor.\n",
    "- $ \\lambda $ is the tuning parameter that controls the strength of the penalty; $ \\lambda \\geq 0 $.\n",
    "\n",
    "In this code, `alpha` is the regularization strength \\( \\lambda \\). Adjusting `alpha` changes the strength of the regularization penalty. A larger `alpha` enforces stronger regularization (leading to smaller coefficients), and a smaller `alpha` tends towards a model similar to linear regression.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "- **Choosing Alpha**: Selecting the right value of `alpha` is crucial. It can be done using cross-validation techniques like `RidgeCV`.\n",
    "- **Standardization**: It's often recommended to standardize the predictors before applying ridge regression.\n",
    "- **Bias-Variance Tradeoff**: Ridge regression balances the bias-variance tradeoff in model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [1.5 1.5]\n",
      "Intercept: 3.5000000000000018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[0, 1], [1, 2], [2, 3], [3, 4]])\n",
    "# Target values\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "\n",
    "# Linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", lr.coef_)\n",
    "# Intercept\n",
    "print(\"Intercept:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [1.42857143 1.42857143]\n",
      "Intercept: 3.785714285714284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[0, 1], [1, 2], [2, 3], [3, 4]])\n",
    "# Target values\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "\n",
    "# Ridge Regression Model\n",
    "ridge_reg = Ridge(alpha=0.5)  # alpha is the equivalent of lambda in the formula\n",
    "ridge_reg.fit(X, y)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", ridge_reg.coef_)\n",
    "# Intercept\n",
    "print(\"Intercept:\", ridge_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression, which stands for `Least Absolute Shrinkage and Selection Operator`, is a type of linear regression that uses shrinkage. Shrinkage here means that the data values are shrunk towards a central point, like the mean. The lasso technique encourages simple, sparse models (i.e., models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "### Key Features of Lasso Regression:\n",
    "\n",
    "1. **Regularization Term**: The key characteristic of Lasso Regression is that it adds an L1 penalty to the regression model, which is the absolute value of the magnitude of the coefficients. The cost function for Lasso regression is:\n",
    "\n",
    "   $$ \\text{Minimize } \\sum*{i=1}^{n} (y_i - \\sum*{j=1}^{p} x*{ij} \\beta_j)^2 + \\lambda \\sum*{j=1}^{p} |\\beta_j| $$\n",
    "\n",
    "   where $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "2. **Feature Selection**: One of the advantages of lasso regression over ridge regression is that it can result in sparse models with few coefficients; some coefficients can become exactly zero and be eliminated from the model. This property is called automatic feature selection and is a form of embedded method.\n",
    "\n",
    "3. **Parameter Tuning**: The strength of the L1 penalty is determined by a parameter, typically denoted as alpha or lambda. Selecting a good value for this parameter is crucial and is typically done using cross-validation.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**: Similar to ridge regression, lasso also manages the bias-variance tradeoff in model training. Increasing the regularization strength increases bias but decreases variance, potentially leading to better generalization on unseen data.\n",
    "\n",
    "5. **Scaling**: Before applying lasso, it is recommended to scale/normalize the data as lasso is sensitive to the scale of input features.\n",
    "\n",
    "### Implementation in Scikit-Learn:\n",
    "\n",
    "Lasso regression can be implemented using the `Lasso` class from Scikit-Learn's `linear_model` module. Here's a basic example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Linear Regression: 0.0111837651150914\n",
      "MSE of Lasso: 0.3847492638484584\n",
      "MSE of Ridge: 0.05090866185226863\n",
      "\n",
      "99.99996380855798 % 99.99995220436705 %\n",
      "99.9984160793672 % 99.99995220436705 % \n",
      "99.9997977193482 % 99.99995220436705 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate some regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=15, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Lasso regression object\n",
    "lr = LinearRegression()\n",
    "lasso = Lasso(alpha=0.2)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"MSE of Linear Regression:\", mean_squared_error(y_test, y_pred_lr))\n",
    "print(\"MSE of Lasso:\", mean_squared_error(y_test, y_pred_lasso))\n",
    "print(\"MSE of Ridge:\", mean_squared_error(y_test, y_pred_ridge))\n",
    "\n",
    "#score \n",
    "print(f\"\\n{lr.score(X_train, y_train)*100} % { lr.score(X_test, y_test)*100} %\")\n",
    "print(f\"{lasso.score(X_train, y_train)*100} % { lr.score(X_test, y_test)*100} % \")\n",
    "print(f\"{ridge.score(X_train, y_train) *100} % { lr.score(X_test, y_test)*100} %\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
